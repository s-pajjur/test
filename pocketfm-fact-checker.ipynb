{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3844c46-f752-4290-9a52-35ef45ed5320",
   "metadata": {},
   "source": [
    "### Installs and Upgrades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9c720-0bc3-470a-8af9-25b522084cb5",
   "metadata": {},
   "source": [
    "need to do the following installations and updates\n",
    "\n",
    "    apt-get update\n",
    "    apt-get upgrade\n",
    "    pip uninstall -y langchain langchain-community langchain-experimental langchain-core langchain-aws langgraph\n",
    "    pip install \"langchain-core>=0.3.27,<0.4.0\"\n",
    "    pip install \"langchain>=0.3.13\"\n",
    "    pip install \"langchain-community>=0.3.13\"\n",
    "    pip install \"langgraph>=0.0.25\"\n",
    "    pip install boto3\n",
    "    pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fed4f8-ac7d-4d24-a2d6-81d4c942315c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f80039-ab13-4a57-9fb3-1cda5f4c7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import Graph, StateGraph\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import wikipedia\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d61df-092d-4873-ab5a-f5b957cb7365",
   "metadata": {},
   "source": [
    "### FactChecker Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5039e-b04c-4862-b659-2dc8fc8cd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactCheckAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # create a callback manager\n",
    "        callbacks = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "        # initialize the Bedrock client\n",
    "        bedrock_client = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name='us-east-1'\n",
    "        )\n",
    "\n",
    "        self.llm = BedrockChat(\n",
    "            model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "            client=bedrock_client,\n",
    "            callback_manager=callbacks,\n",
    "            model_kwargs={\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 4096,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_fact_claims(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract potential fact claims from text\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a precise fact-checking assistant. \n",
    "            Extract ONLY the specific factual claims from the input text.\n",
    "            Return each claim on a new line.\n",
    "            DO NOT include any additional text or formatting.\n",
    "            DO NOT include your own commentary or explanations.\n",
    "            ONLY list the actual claims from the input text.\"\"\"),\n",
    "            (\"human\", \"{text}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\"text\": text})\n",
    "        # filter out empty strings and strip whitespace\n",
    "        claims = [claim.strip() for claim in response.content.split('\\n') if claim.strip()]\n",
    "        # additional filtering to remove any meta-text\n",
    "        claims = [claim for claim in claims if not claim.startswith((\"Here are\", \"These are\", \"The following\"))]\n",
    "        return claims\n",
    "\n",
    "    def verify_with_llm_knowledge(self, claim: str) -> Dict[str, Any]:\n",
    "        \"\"\"verify claim using LLM's built-in knowledge\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a precise fact-checking assistant. Using ONLY your built-in knowledge (not external sources):\n",
    "            1. Analyze the claim's accuracy\n",
    "            2. Determine if it's TRUE, FALSE, or UNCERTAIN\n",
    "            3. Provide a brief explanation\n",
    "            4. Rate your confidence level on a scale of 1-5 (5 being highest)\n",
    "\n",
    "            Format your response exactly as:\n",
    "            STATUS: [TRUE/FALSE/UNCERTAIN]\n",
    "            CONFIDENCE: [1-5]\n",
    "            EXPLANATION: [Your explanation]\"\"\"),\n",
    "            (\"human\", f\"Claim: {claim}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.llm\n",
    "        result = chain.invoke({})\n",
    "\n",
    "        return {\n",
    "            \"claim\": claim,\n",
    "            \"verification\": result.content\n",
    "        }\n",
    "\n",
    "    def _generate_search_terms(self, claim: str) -> str:\n",
    "        \"\"\"Generate better search terms for Wikipedia lookup\"\"\"\n",
    "        \"\"\"This is optional and can use LLM or NLP to generate these automatically to improve results.\"\"\"\n",
    "        \"\"\"Hardcoded here for the sake of POC\"\"\"\n",
    "        keywords = {\n",
    "            \"World War II\": [\"World War II\", \"Second World War\", \"WW2\"],\n",
    "            \"Shakespeare\": [\"William Shakespeare\", \"Shakespeare biography\"],\n",
    "            \"Einstein\": [\"Albert Einstein\", \"Einstein physics\"],\n",
    "            \"Great Wall of China\": [\"Great Wall of China\", \"Great Wall visibility from space\"],\n",
    "        }\n",
    "\n",
    "        search_terms = claim\n",
    "        for key, terms in keywords.items():\n",
    "            if key.lower() in claim.lower():\n",
    "                search_terms = terms[0]  # Use the first (most general) term\n",
    "                break\n",
    "\n",
    "        return search_terms\n",
    "\n",
    "    def _wiki_fact_check(self, claim: str) -> Dict[str, Any]:\n",
    "        \"\"\"Wikipedia fact-checking logic\"\"\"\n",
    "        try:\n",
    "            search_terms = self._generate_search_terms(claim)\n",
    "            search_results = wikipedia.search(search_terms)\n",
    "\n",
    "            if search_results:\n",
    "                try:\n",
    "                    for result in search_results[:3]:\n",
    "                        try:\n",
    "                            page = wikipedia.page(result, auto_suggest=False)\n",
    "                            content = page.summary\n",
    "                            break\n",
    "                        except wikipedia.exceptions.DisambiguationError as e:\n",
    "                            try:\n",
    "                                page = wikipedia.page(e.options[0], auto_suggest=False)\n",
    "                                content = page.summary\n",
    "                                break\n",
    "                            except:\n",
    "                                continue\n",
    "                        except:\n",
    "                            continue\n",
    "                    else:\n",
    "                        return {\n",
    "                            \"claim\": claim,\n",
    "                            \"verification\": \"STATUS: UNCERTAIN\\nEXPLANATION: Could not find reliable Wikipedia information to verify this claim.\",\n",
    "                            \"source\": None\n",
    "                        }\n",
    "\n",
    "                    verification_prompt = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"\"\"\n",
    "                        You are a precise fact-checking assistant. Follow this protocol:\n",
    "                        1. Carefully analyze the claim against the provided Wikipedia content\n",
    "                        2. Determine if the claim is TRUE, FALSE, or UNCERTAIN\n",
    "                        3. Provide a clear, concise explanation with specific references\n",
    "                        4. Format your response as:\n",
    "                           STATUS: [TRUE/FALSE/UNCERTAIN]\n",
    "                           EXPLANATION: [Your explanation]\n",
    "                        5. Keep explanations concise and focused on the specific claim\n",
    "                        \"\"\"),\n",
    "                        (\"human\", f\"Claim: {claim}\\n\\nWikipedia Content: {content}\")\n",
    "                    ])\n",
    "\n",
    "                    verification_chain = verification_prompt | self.llm\n",
    "                    result = verification_chain.invoke({})\n",
    "\n",
    "                    return {\n",
    "                        \"claim\": claim,\n",
    "                        \"verification\": result.content,\n",
    "                        \"source\": page.title\n",
    "                    }\n",
    "\n",
    "                except Exception as e:\n",
    "                    return {\n",
    "                        \"claim\": claim,\n",
    "                        \"verification\": f\"STATUS: UNCERTAIN\\nEXPLANATION: Error accessing Wikipedia content: {str(e)}\",\n",
    "                        \"source\": None\n",
    "                    }\n",
    "\n",
    "            return {\n",
    "                \"claim\": claim,\n",
    "                \"verification\": \"STATUS: UNCERTAIN\\nEXPLANATION: No reliable Wikipedia source found to verify this claim.\",\n",
    "                \"source\": None\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"claim\": claim,\n",
    "                \"verification\": f\"STATUS: ERROR\\nEXPLANATION: {str(e)}\",\n",
    "                \"source\": None\n",
    "            }\n",
    "\n",
    "    def fact_check_claim(self, claim: str) -> Dict[str, Any]:\n",
    "        \"\"\"Fact-check a claim using both LLM knowledge and Wikipedia\"\"\"\n",
    "        # get LLM knowledge-based verification\n",
    "        llm_verification = self.verify_with_llm_knowledge(claim)\n",
    "\n",
    "        # get Wikipedia-based verification\n",
    "        wiki_verification = self._wiki_fact_check(claim)\n",
    "\n",
    "        return {\n",
    "            \"claim\": claim,\n",
    "            \"llm_verification\": llm_verification[\"verification\"],\n",
    "            \"wiki_verification\": wiki_verification[\"verification\"],\n",
    "            \"wiki_source\": wiki_verification[\"source\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641d533-fc62-44da-939c-6a422bacbf7b",
   "metadata": {},
   "source": [
    "### Build LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eecdaf1-98fd-4c5b-8b46-e4ecd07046a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fact_checking_workflow():\n",
    "\n",
    "    workflow = StateGraph(dict)\n",
    "\n",
    "    fact_check_agent = FactCheckAgent()\n",
    "\n",
    "    def extract_claims(state: dict):\n",
    "        \"\"\"Extract claims from input text\"\"\"\n",
    "        text = state.get('text', '')\n",
    "        claims = fact_check_agent.extract_fact_claims(text)\n",
    "        return {\"claims\": claims}\n",
    "\n",
    "    def verify_claims(state: dict):\n",
    "        \"\"\"Verify extracted claims\"\"\"\n",
    "        claims = state.get('claims', [])\n",
    "        fact_checks = [fact_check_agent.fact_check_claim(claim) for claim in claims]\n",
    "        return {\"fact_checks\": fact_checks}\n",
    "\n",
    "    def generate_report(state: dict):\n",
    "        fact_checks = state.get('fact_checks', [])\n",
    "        report = \"ðŸ“‹ Fact Check Report\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "\n",
    "        for i, check in enumerate(fact_checks, 1):\n",
    "            report += f\"Claim #{i}: {check['claim']}\\n\"\n",
    "            report += f\"{'='*30}\\n\"\n",
    "            report += \"ðŸ¤– LLM Knowledge Verification:\\n\"\n",
    "            report += f\"{check['llm_verification']}\\n\\n\"\n",
    "            report += \"ðŸ“š Wikipedia Verification:\\n\"\n",
    "            report += f\"{check['wiki_verification']}\\n\"\n",
    "            report += f\"Source: {check['wiki_source'] or 'No specific source'}\\n\"\n",
    "            report += f\"{'-'*50}\\n\\n\"\n",
    "\n",
    "        return {\"report\": report, \"completed\": True}\n",
    "\n",
    "    # add nodes\n",
    "    workflow.add_node(\"extract_claims\", extract_claims)\n",
    "    workflow.add_node(\"verify_claims\", verify_claims)\n",
    "    workflow.add_node(\"generate_report\", generate_report)\n",
    "\n",
    "    # add edges\n",
    "    workflow.set_entry_point(\"extract_claims\")\n",
    "    workflow.add_edge(\"extract_claims\", \"verify_claims\")\n",
    "    workflow.add_edge(\"verify_claims\", \"generate_report\")\n",
    "\n",
    "    # define end condition\n",
    "    def check_end(state):\n",
    "        return \"end\" if state.get(\"completed\", False) else \"generate_report\"\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_report\",\n",
    "        check_end,\n",
    "        {\n",
    "            \"end\": \"end\",\n",
    "            \"generate_report\": \"generate_report\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # add end node\n",
    "    workflow.add_node(\"end\", lambda x: x)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cf006-a8e6-48f9-9f2c-362d9cac55ca",
   "metadata": {},
   "source": [
    "### Main Driver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be5de9-77cb-4a6e-aa7b-9b9cb4fb1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    workflow = create_fact_checking_workflow()\n",
    "\n",
    "    text = \"\"\"\n",
    "    World War II happened in the 15th century. \n",
    "    Shakespeare was a famous painter from England. \n",
    "    Albert Einstein developed the theory of relativity.\n",
    "    The Great Wall of China is visible from space.\n",
    "    Paris is a city in Italy.\n",
    "    Shahrukh Khan was the lead actor in the movie Sholay\n",
    "    \"\"\"\n",
    "\n",
    "    result = workflow.invoke({\"text\": text})\n",
    "    print(result['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea805879-5a68-46db-8789-8a6b0d4c0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43d827-a995-4eac-8f5f-f73293884a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
